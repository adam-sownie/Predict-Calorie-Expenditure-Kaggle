{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e826f7-8e9b-4af6-9514-289ea4161b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle API Dataset Download\n",
    "# This section handles downloading and accessing the dataset for the Playground Series S5E5 competition\n",
    "\n",
    "import os\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip --user\n",
    "!{sys.executable} -m pip install -r requirements.txt\n",
    "!{sys.executable} -m pip install cudf\n",
    "!{sys.executable} -m pip install cuml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import cudf  # RAPIDS - GPU-accelerated pandas\n",
    "import cuml  # RAPIDS - GPU-accelerated scikit-learn\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# This cell detects whether we're running on Kaggle or locally\n",
    "IN_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    # If running on Kaggle, the data is already available in the /kaggle/input directory\n",
    "    print(\"Running on Kaggle - dataset already available\")\n",
    "\n",
    "    # Competition data paths\n",
    "    BASE_DIR = '/kaggle/input/playground-series-s5e5'\n",
    "\n",
    "else:\n",
    "    # If running locally, we need to download the data via the Kaggle API\n",
    "    print(\"Running locally - downloading data via Kaggle API\")\n",
    "\n",
    "    # First, check if kaggle module is installed\n",
    "    try:\n",
    "        import kaggle\n",
    "    except ImportError:\n",
    "        print(\"Kaggle API not found. Installing...\")\n",
    "        !pip install kaggle\n",
    "        import kaggle\n",
    "\n",
    "    # Create directory for data if it doesn't exist\n",
    "    os.makedirs('kaggle_data', exist_ok=True)\n",
    "\n",
    "    # Download competition data\n",
    "    # Note: You need to have your Kaggle API credentials in ~/.kaggle/kaggle.json\n",
    "    # If not already set up, run the following commands in a cell:\n",
    "    \"\"\"\n",
    "    # Run this if you haven't set up Kaggle API credentials:\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !echo '{\"username\":\"YOUR_USERNAME\",\"key\":\"YOUR_KEY\"}' > ~/.kaggle/kaggle.json\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    \"\"\"\n",
    "\n",
    "    # Download all competition files\n",
    "    !kaggle competitions download -c playground-series-s5e5 -p kaggle_data\n",
    "\n",
    "    # Unzip the downloaded files\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile('kaggle_data/playground-series-s5e5.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('kaggle_data')\n",
    "\n",
    "    print(\"Dataset downloaded successfully!\")\n",
    "\n",
    "    # Set the base directory for data access\n",
    "    BASE_DIR = 'kaggle_data'\n",
    "\n",
    "# Now let's define paths to access the files in a consistent way\n",
    "# This will work both on Kaggle and locally\n",
    "\n",
    "train_path = os.path.join(BASE_DIR, 'train.csv')\n",
    "test_path = os.path.join(BASE_DIR, 'test.csv')\n",
    "sample_submission_path = os.path.join(BASE_DIR, 'sample_submission.csv')\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(\"\\n--- Dataset Information ---\")\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "\n",
    "# Display a few rows of the training data\n",
    "print(\"\\n--- First few rows of training data ---\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26677b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-Accelerated Calorie Prediction Model Training\n",
    "\n",
    "## 1. Import necessary libraries\n",
    "\n",
    "# GPU-accelerated libraries\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"GPU not available, using CPU instead\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "## 2. Create the dataset from the provided data\n",
    "df = train_df  # Assuming train_df is already loaded\n",
    "\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Visualize the target variable distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['Calories'], bins=10, edgecolor='black')\n",
    "plt.title('Distribution of Calories')\n",
    "plt.xlabel('Calories')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Convert categorical to numerical first\n",
    "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = df.drop('id', axis=1).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## 4. GPU-accelerated Feature Engineering and Preprocessing\n",
    "# Try to use RAPIDS cuDF if available, otherwise fall back to pandas\n",
    "try:\n",
    "    # Convert pandas DataFrame to cuDF DataFrame for GPU acceleration\n",
    "    gpu_df = cudf.DataFrame.from_pandas(df)\n",
    "    print(\"Using RAPIDS cuDF for GPU acceleration\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X_gpu = gpu_df.drop(['id', 'Calories'], axis=1)\n",
    "    y_gpu = gpu_df['Calories']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train_gpu, X_test_gpu, y_train_gpu, y_test_gpu = train_test_split(\n",
    "        X_gpu, y_gpu, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Feature scaling with cuML\n",
    "    scaler = cuml.preprocessing.StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_gpu)\n",
    "    X_test_scaled = scaler.transform(X_test_gpu)\n",
    "    \n",
    "    # Convert back to numpy for compatibility with some operations\n",
    "    X_train_np = X_train_scaled.to_numpy()\n",
    "    X_test_np = X_test_scaled.to_numpy()\n",
    "    y_train_np = y_train_gpu.to_numpy()\n",
    "    y_test_np = y_test_gpu.to_numpy()\n",
    "    \n",
    "except (ImportError, AttributeError) as e:\n",
    "    print(f\"RAPIDS not available or error: {e}\")\n",
    "    print(\"Falling back to CPU preprocessing\")\n",
    "    \n",
    "    # Separate features and target using regular pandas\n",
    "    X = df.drop(['id', 'Calories'], axis=1)\n",
    "    y = df['Calories']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_np = scaler.fit_transform(X_train)\n",
    "    X_test_np = scaler.transform(X_test)\n",
    "    y_train_np = y_train.values\n",
    "    y_test_np = y_test.values\n",
    "    \n",
    "    # Store column names for later use\n",
    "    feature_names = X.columns\n",
    "\n",
    "\n",
    "## 5. PyTorch Neural Network Model (GPU-Accelerated)\n",
    "\n",
    "# Custom dataset class for PyTorch\n",
    "class CalorieDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).reshape(-1, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CalorieDataset(X_train_np, y_train_np)\n",
    "test_dataset = CalorieDataset(X_test_np, y_test_np)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define neural network model\n",
    "class CaloriePredictor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CaloriePredictor, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_dim = X_train_np.shape[1]\n",
    "model = CaloriePredictor(input_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=100):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f'Training completed in {training_time:.2f} seconds')\n",
    "    return train_losses\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Move predictions and actuals to CPU for numpy conversion\n",
    "            pred = outputs.cpu().numpy()\n",
    "            actual = targets.cpu().numpy()\n",
    "            \n",
    "            predictions.extend(pred.flatten().tolist())\n",
    "            actuals.extend(actual.flatten().tolist())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'actuals': actuals,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'avg_loss': running_loss / len(data_loader)\n",
    "    }\n",
    "\n",
    "## 6. Train the PyTorch model\n",
    "print(\"\\nTraining PyTorch Neural Network on GPU...\")\n",
    "train_losses = train_model(model, train_loader, criterion, optimizer, epochs=100)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "## 7. Evaluate the PyTorch model\n",
    "print(\"\\nEvaluating model performance...\")\n",
    "train_results = evaluate_model(model, train_loader, criterion)\n",
    "test_results = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "print(\"\\nTraining Results:\")\n",
    "print(f\"MSE: {train_results['mse']:.2f}\")\n",
    "print(f\"RMSE: {train_results['rmse']:.2f}\")\n",
    "print(f\"MAE: {train_results['mae']:.2f}\")\n",
    "print(f\"R2 Score: {train_results['r2']:.2f}\")\n",
    "\n",
    "print(\"\\nTesting Results:\")\n",
    "print(f\"MSE: {test_results['mse']:.2f}\")\n",
    "print(f\"RMSE: {test_results['rmse']:.2f}\")\n",
    "print(f\"MAE: {test_results['mae']:.2f}\")\n",
    "print(f\"R2 Score: {test_results['r2']:.2f}\")\n",
    "\n",
    "## 8. Visualize predictions\n",
    "def plot_actual_vs_predicted(actuals, predictions, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(actuals, predictions)\n",
    "    plt.plot([min(actuals), max(actuals)], [min(actuals), max(actuals)], 'r--')\n",
    "    plt.xlabel('Actual Calories')\n",
    "    plt.ylabel('Predicted Calories')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_actual_vs_predicted(\n",
    "    test_results['actuals'], \n",
    "    test_results['predictions'], \n",
    "    'Actual vs Predicted Calories (GPU PyTorch Model)'\n",
    ")\n",
    "\n",
    "## 9. Feature Importance Analysis with GPU-Accelerated RAPIDS Random Forest (optional)\n",
    "try:\n",
    "    print(\"\\nCalculating feature importance with RAPIDS Random Forest...\")\n",
    "    from cuml.ensemble import RandomForestRegressor as cuRFR\n",
    "    \n",
    "    # Train a GPU-accelerated Random Forest\n",
    "    rf_model = cuRFR(n_estimators=100, max_depth=10, random_state=42)\n",
    "    rf_model.fit(X_train_scaled, y_train_gpu)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importances = rf_model.feature_importances_\n",
    "    feature_names = X_gpu.columns\n",
    "    \n",
    "    # Sort features by importance\n",
    "    indices = np.argsort(importances.to_array())[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title('Feature Importance - RAPIDS Random Forest')\n",
    "    plt.bar(range(X_train_scaled.shape[1]), importances[indices], align='center')\n",
    "    plt.xticks(range(X_train_scaled.shape[1]), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFeature Importance:\")\n",
    "    for i in indices:\n",
    "        print(f\"{feature_names[i]}: {importances[i]:.4f}\")\n",
    "        \n",
    "except (ImportError, AttributeError, NameError) as e:\n",
    "    print(f\"Could not run RAPIDS Random Forest: {e}\")\n",
    "    print(\"Skipping feature importance analysis\")\n",
    "\n",
    "## 10. Function to predict calories for new data using the PyTorch model\n",
    "def predict_calories(data_dict, model, scaler, device):\n",
    "    \"\"\"\n",
    "    Predict calories for new data using the PyTorch model.\n",
    "    \n",
    "    Parameters:\n",
    "    data_dict (dict): Dictionary with keys as feature names and values\n",
    "    model: Trained PyTorch model\n",
    "    scaler: Fitted scaler\n",
    "    device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "    float: Predicted calories\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame\n",
    "    new_data = pd.DataFrame([data_dict])\n",
    "    \n",
    "    # Convert categorical to numerical\n",
    "    if 'Sex' in new_data.columns:\n",
    "        new_data['Sex'] = new_data['Sex'].map({'male': 1, 'female': 0})\n",
    "    \n",
    "    try:\n",
    "        # Try to use cuDF for GPU acceleration\n",
    "        new_data_gpu = cudf.DataFrame.from_pandas(new_data)\n",
    "        new_data_scaled = scaler.transform(new_data_gpu)\n",
    "        new_data_np = new_data_scaled.to_numpy()\n",
    "    except (ImportError, AttributeError, NameError):\n",
    "        # Fall back to CPU\n",
    "        new_data_scaled = scaler.transform(new_data)\n",
    "        new_data_np = new_data_scaled\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    new_data_tensor = torch.tensor(new_data_np, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        prediction = model(new_data_tensor).item()\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Example usage\n",
    "example_data = {\n",
    "    'Sex': 'male',\n",
    "    'Age': 30,\n",
    "    'Height': 175,\n",
    "    'Weight': 70,\n",
    "    'Duration': 20,\n",
    "    'Heart_Rate': 95,\n",
    "    'Body_Temp': 40\n",
    "}\n",
    "\n",
    "predicted_calories = predict_calories(example_data, model, scaler, device)\n",
    "print(f\"\\nPredicted calories for example data: {predicted_calories:.2f}\")\n",
    "\n",
    "## 11. Save the model (optional)\n",
    "# Save PyTorch model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'calorie_prediction_model_gpu.pt')\n",
    "\n",
    "print(\"\\nModel saved successfully as 'calorie_prediction_model_gpu.pt'\")\n",
    "\n",
    "# Save the scaler (you may need to convert it to CPU if it's a RAPIDS scaler)\n",
    "try:\n",
    "    import joblib\n",
    "    joblib.dump(scaler, 'scaler_gpu.pkl')\n",
    "    print(\"Scaler saved successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save scaler: {e}\")\n",
    "\n",
    "## 12. Conclusion\n",
    "print(\"\\nGPU-Accelerated Model Training Summary:\")\n",
    "print(f\"Model: PyTorch Neural Network\")\n",
    "print(f\"Sample size: {len(df)} records\")\n",
    "print(f\"GPU used: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None (CPU used)'}\")\n",
    "print(f\"Model performance (R2 Score): {test_results['r2']:.2f}\")\n",
    "print(\"\\nAdvantages of GPU implementation:\")\n",
    "print(\"- Faster training, especially with larger datasets\")\n",
    "print(\"- Potential for more complex models\")\n",
    "print(\"- Efficient batch processing\")\n",
    "print(\"\\nLimitations:\")\n",
    "print(\"- Requires compatible hardware\")\n",
    "print(\"- Some libraries may need specific versions/configurations\")\n",
    "print(\"- Small datasets may not show significant speedup\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
