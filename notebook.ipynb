{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e826f7-8e9b-4af6-9514-289ea4161b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle API Dataset Download\n",
    "# This section handles downloading and accessing the dataset for the Playground Series S5E5 competition\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# This cell detects whether we're running on Kaggle or locally\n",
    "IN_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    # If running on Kaggle, the data is already available in the /kaggle/input directory\n",
    "    print(\"Running on Kaggle - dataset already available\")\n",
    "\n",
    "    # Competition data paths\n",
    "    BASE_DIR = '/kaggle/input/playground-series-s5e5'\n",
    "\n",
    "else:\n",
    "    # If running locally, we need to download the data via the Kaggle API\n",
    "    print(\"Running locally - downloading data via Kaggle API\")\n",
    "\n",
    "    # First, check if kaggle module is installed\n",
    "    try:\n",
    "        import kaggle\n",
    "    except ImportError:\n",
    "        print(\"Kaggle API not found. Installing...\")\n",
    "        !pip install kaggle\n",
    "        import kaggle\n",
    "\n",
    "    # Create directory for data if it doesn't exist\n",
    "    os.makedirs('kaggle_data', exist_ok=True)\n",
    "\n",
    "    # Download competition data\n",
    "    # Note: You need to have your Kaggle API credentials in ~/.kaggle/kaggle.json\n",
    "    # If not already set up, run the following commands in a cell:\n",
    "    \"\"\"\n",
    "    # Run this if you haven't set up Kaggle API credentials:\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !echo '{\"username\":\"YOUR_USERNAME\",\"key\":\"YOUR_KEY\"}' > ~/.kaggle/kaggle.json\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    \"\"\"\n",
    "\n",
    "    # Download all competition files\n",
    "    !kaggle competitions download -c playground-series-s5e5 -p kaggle_data\n",
    "\n",
    "    # Unzip the downloaded files\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile('kaggle_data/playground-series-s5e5.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('kaggle_data')\n",
    "\n",
    "    print(\"Dataset downloaded successfully!\")\n",
    "\n",
    "    # Set the base directory for data access\n",
    "    BASE_DIR = 'kaggle_data'\n",
    "\n",
    "# Now let's define paths to access the files in a consistent way\n",
    "# This will work both on Kaggle and locally\n",
    "train_path = os.path.join(BASE_DIR, 'train.csv')\n",
    "test_path = os.path.join(BASE_DIR, 'test.csv')\n",
    "sample_submission_path = os.path.join(BASE_DIR, 'sample_submission.csv')\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(\"\\n--- Dataset Information ---\")\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "\n",
    "# Display a few rows of the training data\n",
    "print(\"\\n--- First few rows of training data ---\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35923aa6-6c3a-47eb-bfe5-7d446f9c1e87",
   "metadata": {},
   "source": [
    "From an earlier run, we determined this feature importance.  Commenting out everything with extremely low importance.\n",
    "\n",
    "Feature Importance:\n",
    "\n",
    "Age_Based_Intensity: 0.6814467310905457\n",
    "Temp_Heart_Interaction_Duration: 0.18323101103305817\n",
    "Duration: 0.053071144968271255\n",
    "BMI_Intensity: 0.038230136036872864\n",
    "TotalTemp: 0.030490227043628693\n",
    "Female_Intensity: 0.0040832036174833775\n",
    "HR_Based_Calories: 0.0018469374626874924\n",
    "Sex_male: 0.0016337857814505696\n",
    "Age_Based_HR_Percent: 0.0015912051312625408\n",
    "Male_Intensity: 0.0014280130853876472\n",
    "Temp_Heart_Interaction: 0.0010874277213588357\n",
    "Heart_Rate: 0.0005569902714341879\n",
    "Calories_Per_Minute: 0.00046593035222031176\n",
    "Est_Max_HR: 0.00021114895935170352\n",
    "VO2_Calories: 0.00013731316721532494\n",
    "Age: 0.00012986226647626609\n",
    "Body_Temp: 8.561972208553925e-05\n",
    "Weight: 7.448684482369572e-05\n",
    "HR_Times_Weight_Times_Duration: 6.925689376657829e-05\n",
    "HR_Times_Weight: 4.754386463901028e-05\n",
    "Height: 4.337794962339103e-05\n",
    "BMI: 3.855152681353502e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb29f79-0631-4826-af61-b7bb52deb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    # Original preprocessing\n",
    "    if 'id' in df.columns:\n",
    "        df = df.drop('id', axis=1)\n",
    "    df['Intensity'] = df['Heart_Rate'] * df['Duration']\n",
    "    df['TotalTemp'] = df['Body_Temp'] * df['Duration']\n",
    "    df['Sex'] = pd.Categorical(df.Sex)\n",
    "\n",
    "    df['Temp_Heart_Interaction'] = df['Body_Temp'] * df['Heart_Rate']\n",
    "    # df['HR_Times_Weight'] = df['Heart_Rate'] * df['Weight']\n",
    "    df['Est_Max_HR'] = 220 - df['Age']\n",
    "    df['Age_Based_HR_Percent'] = df['Heart_Rate'] / df['Est_Max_HR']\n",
    "    df['BMI'] = df['Weight'] / (df['Height'] ** 2)\n",
    "\n",
    "    df['Temp_Heart_Interaction_Duration'] = df['Temp_Heart_Interaction'] * df['Duration']\n",
    "    # df['HR_Times_Weight_Times_Duration'] = df['HR_Times_Weight'] * df['Duration']\n",
    "    df['Age_Based_Intensity'] = df['Age_Based_HR_Percent'] * df['Duration']\n",
    "    df['BMI_Intensity'] = df['BMI'] * df['Intensity']\n",
    "\n",
    "    # df['Male_Intensity'] = (df.Sex=='male')*df.Age_Based_Intensity\n",
    "    # df['Female_Intensity'] = (df.Sex=='female')*df.Age_Based_Intensity\n",
    "    \n",
    "    # Calculate VO₂-Based Calorie Estimate\n",
    "    # Step 1: Estimate VO₂_max (mL/kg/min) - using a simplified age-based estimate\n",
    "    # A common simple estimate is 45.2 - 0.35*Age for men, slightly lower for women\n",
    "    # We'll use an average approach of 40 - 0.25*Age for simplicity\n",
    "    vo2_max = 40 - 0.25 * df['Age']\n",
    "    \n",
    "    # Step 2: Calculate estimated VO₂ during exercise based on heart rate\n",
    "    # Assuming HR is linearly related to VO₂ consumption (simplified model)\n",
    "    # MaxHR estimated as 208 - 0.7 * Age\n",
    "    max_hr = 208 - 0.7 * df['Age']\n",
    "    hr_percentage = df['Heart_Rate'] / max_hr\n",
    "    vo2_estimate = vo2_max * hr_percentage\n",
    "    \n",
    "    # Step 3: Calculate total O₂ consumed in liters\n",
    "    # Total O₂ = VO₂ (mL/kg/min) × Weight(kg) × Duration(min) / 1000\n",
    "    total_o2_consumed = vo2_estimate * df['Weight'] * df['Duration'] / 1000\n",
    "    \n",
    "    # Step 4: Convert O₂ to calories (5 kcal per liter of O₂)\n",
    "    df['VO2_Calories'] = total_o2_consumed * 5\n",
    "\n",
    "    is_male = df['Sex'] == 'male'\n",
    "    \n",
    "    # Calculate calories per minute based on the formulas\n",
    "    \"\"\"\n",
    "    male_calories_per_min = (-55.0969 + 0.6309 * df['Heart_Rate'] + \n",
    "                            0.1988 * df['Weight'] + \n",
    "                            0.2017 * df['Age']) / 4.184\n",
    "    \n",
    "    female_calories_per_min = (-20.4022 + 0.4472 * df['Heart_Rate'] - \n",
    "                              0.1263 * df['Weight'] + \n",
    "                              0.074 * df['Age']) / 4.184\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assign the appropriate calculation based on sex\n",
    "    #df['Calories_Per_Minute'] = np.where(is_male, male_calories_per_min, female_calories_per_min)\n",
    "    \n",
    "    # Calculate total calories based on duration\n",
    "    #df['HR_Based_Calories'] = df['Calories_Per_Minute'] * df['Duration']\n",
    "\n",
    "    if 'Intensity' in df.columns:\n",
    "        df = df.drop('Intensity', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f395019-1e27-45b2-b5a3-afc1fa7aeda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_normalization_method(df, plot=True, sample_rows=None):\n",
    "    \"\"\"\n",
    "    Analyzes each numeric column in a dataframe and provides stats to help decide\n",
    "    between log transformation and division by max normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe to analyze\n",
    "    plot : bool, default=True\n",
    "        Whether to plot histograms of the distributions\n",
    "    sample_rows : int, default=None\n",
    "        Number of rows to sample for analysis (useful for large dataframes)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A dataframe with normalization recommendations\n",
    "    \"\"\"\n",
    "    if sample_rows is not None and sample_rows < len(df):\n",
    "        df_sample = df.sample(sample_rows, random_state=42)\n",
    "    else:\n",
    "        df_sample = df\n",
    "    \n",
    "    # Select only numeric columns\n",
    "    numeric_df = df_sample.select_dtypes(include=['number'])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if plot:\n",
    "        # Determine grid size for subplots\n",
    "        n_cols = min(3, len(numeric_df.columns))\n",
    "        n_rows = int(np.ceil(len(numeric_df.columns) / n_cols))\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "        if n_rows == 1 and n_cols == 1:\n",
    "            axes = np.array([axes])\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numeric_df.columns):\n",
    "        data = numeric_df[col].dropna()\n",
    "        \n",
    "        # Skip if column has no data\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Basic statistics\n",
    "        min_val = data.min()\n",
    "        max_val = data.max()\n",
    "        range_val = max_val - min_val\n",
    "        mean_val = data.mean()\n",
    "        median_val = data.median()\n",
    "        std_val = data.std()\n",
    "        \n",
    "        # Check for zeros and negative values\n",
    "        has_zeros = (data == 0).any()\n",
    "        has_negatives = (data < 0).any()\n",
    "        \n",
    "        # Calculate skewness and kurtosis\n",
    "        skewness = stats.skew(data)\n",
    "        kurtosis = stats.kurtosis(data)\n",
    "        \n",
    "        # Calculate range ratio (max/min) for detecting orders of magnitude\n",
    "        if min_val > 0:\n",
    "            range_ratio = max_val / min_val\n",
    "        else:\n",
    "            range_ratio = np.nan\n",
    "            \n",
    "        # Calculate IQR and detect outliers\n",
    "        q1 = data.quantile(0.25)\n",
    "        q3 = data.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        outlier_ratio = ((data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr))).mean()\n",
    "        \n",
    "        # Generate recommendation\n",
    "        if has_negatives:\n",
    "            recommendation = \"Shift and normalize (can't use log directly)\"\n",
    "        elif has_zeros:\n",
    "            if skewness > 1.0 or range_ratio > 100:\n",
    "                recommendation = \"Log(x+1) transformation\"\n",
    "            else:\n",
    "                recommendation = \"Division by max\"\n",
    "        else:  # Positive values only\n",
    "            if skewness > 1.0 or range_ratio > 100:\n",
    "                recommendation = \"Log transformation\"\n",
    "            else:\n",
    "                recommendation = \"Division by max\"\n",
    "                \n",
    "        # Confidence score (simple heuristic)\n",
    "        confidence = 0\n",
    "        if abs(skewness) > 2:  # Highly skewed\n",
    "            confidence += 2\n",
    "        elif abs(skewness) > 1:  # Moderately skewed\n",
    "            confidence += 1\n",
    "            \n",
    "        if range_ratio > 1000:  # Very wide range\n",
    "            confidence += 2\n",
    "        elif range_ratio > 100:  # Wide range\n",
    "            confidence += 1\n",
    "            \n",
    "        if outlier_ratio > 0.05:  # Many outliers\n",
    "            confidence += 1\n",
    "            \n",
    "        confidence = min(confidence, 5)  # Cap at 5\n",
    "        \n",
    "        # Plot if requested\n",
    "        if plot and i < len(axes):\n",
    "            ax = axes[i]\n",
    "            sns.histplot(data, ax=ax, kde=True)\n",
    "            ax.set_title(f\"{col}\\nRecommendation: {recommendation}\")\n",
    "            ax.set_xlabel(f\"Skewness: {skewness:.2f}, Range Ratio: {range_ratio:.1f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'Column': col,\n",
    "            'Min': min_val,\n",
    "            'Max': max_val,\n",
    "            'Mean': mean_val,\n",
    "            'Median': median_val,\n",
    "            'Std': std_val,\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurtosis,\n",
    "            'Range_Ratio': range_ratio,\n",
    "            'Has_Zeros': has_zeros,\n",
    "            'Has_Negatives': has_negatives,\n",
    "            'Outlier_Ratio': outlier_ratio,\n",
    "            'Recommendation': recommendation,\n",
    "            'Confidence': confidence\n",
    "        })\n",
    "    \n",
    "    if plot:\n",
    "        # Hide empty subplots\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n--- Normalization Recommendation Summary ---\")\n",
    "    for _, row in result_df.iterrows():\n",
    "        print(f\"\\n{row['Column']}:\")\n",
    "        print(f\"  Range: {row['Min']:.4g} to {row['Max']:.4g} (Ratio: {row['Range_Ratio']:.4g})\")\n",
    "        print(f\"  Distribution: Mean={row['Mean']:.4g}, Median={row['Median']:.4g}, Std={row['Std']:.4g}\")\n",
    "        print(f\"  Skewness: {row['Skewness']:.4g}, Kurtosis: {row['Kurtosis']:.4g}\")\n",
    "        print(f\"  Contains: {'Zeros' if row['Has_Zeros'] else 'No zeros'}, {'Negatives' if row['Has_Negatives'] else 'No negatives'}\")\n",
    "        print(f\"  Outliers: {row['Outlier_Ratio']*100:.2f}% of values\")\n",
    "        print(f\"  Recommendation: {row['Recommendation']} (Confidence: {row['Confidence']}/5)\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e704a4-54a5-4bab-86a1-ac34c196836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = preprocess_df(train_df)\n",
    "#result_df = analyze_normalization_method(train_df)\n",
    "#result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd8d55e-20b5-43ac-9bfe-cd5606847163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df_for_nn(df, log_cols, excluded_cols):\n",
    "    \"\"\"\n",
    "    Normalizes all numeric features in a dataframe - using log normalization for\n",
    "    specified columns and division by max for all other numeric columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe containing features to normalize\n",
    "    log_cols : list\n",
    "        List of column names to apply log normalization to\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        A new dataframe with normalized features\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe\n",
    "    normalized_df = df.copy()\n",
    "    \n",
    "    # Get numeric columns\n",
    "    numeric_cols = normalized_df.select_dtypes(include=['number']).columns.tolist()\n",
    "    numeric_cols = [col for col in numeric_cols if col not in excluded_cols]\n",
    "    \n",
    "    # Apply log normalization to specified columns\n",
    "    for col in log_cols:\n",
    "        if col in numeric_cols:\n",
    "            normalized_df[col] = np.log1p(normalized_df[col])  # log(1+x)\n",
    "    \n",
    "    # Apply division by max to all other numeric columns\n",
    "    max_cols = [col for col in numeric_cols if col not in log_cols]\n",
    "    for col in max_cols:\n",
    "        max_val = normalized_df[col].max()\n",
    "        normalized_df[col] = normalized_df[col] / max_val\n",
    "    \n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8455cbb-288d-4890-83c7-21ed8b5e17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_test_df = test_df\n",
    "\n",
    "log_cols = ['VO2_Calories']\n",
    "excluded_cols = ['Calories']\n",
    "\n",
    "train_df = preprocess_df(train_df)\n",
    "test_df = preprocess_df(test_df)\n",
    "\n",
    "nn_train_df = normalize_df_for_nn(train_df, log_cols, excluded_cols)\n",
    "nn_test_df = normalize_df_for_nn(test_df, log_cols, excluded_cols)\n",
    "\n",
    "cats=['Sex']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755ea67-d8db-4f04-b41a-ef7b8416a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a354c0-8462-4a3a-94da-2de2e3b6e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "\n",
    "def train_simplified_tabnet(nn_train_df, nn_test_df, target_col='Calories', cat_features=['Sex']):\n",
    "    print(\"Training simplified TabNet with MSE optimization on log-scale...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original dataframes\n",
    "    train_df = nn_train_df.copy()\n",
    "    test_df = nn_test_df.copy()\n",
    "    \n",
    "    # Apply label encoding to categorical features\n",
    "    label_encoders = {}\n",
    "    for cat_col in cat_features:\n",
    "        if cat_col in train_df.columns:\n",
    "            le = LabelEncoder()\n",
    "            all_values = pd.concat([train_df[cat_col], test_df[cat_col] if cat_col in test_df.columns else pd.Series()])\n",
    "            le.fit(all_values.astype(str))\n",
    "            train_df[cat_col] = le.transform(train_df[cat_col].astype(str))\n",
    "            if cat_col in test_df.columns:\n",
    "                test_df[cat_col] = le.transform(test_df[cat_col].astype(str))\n",
    "            label_encoders[cat_col] = le\n",
    "    \n",
    "    # Split features and target\n",
    "    X = train_df.drop(target_col, axis=1)\n",
    "    y = train_df[target_col]\n",
    "    \n",
    "    # Create train/validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Log-transform targets\n",
    "    y_train_log = np.log1p(np.maximum(0, y_train))\n",
    "    y_val_log = np.log1p(np.maximum(0, y_val))\n",
    "    \n",
    "    # For TabNet format\n",
    "    y_train_log = y_train_log.values.reshape(-1, 1)\n",
    "    y_val_log = y_val_log.values.reshape(-1, 1)\n",
    "    \n",
    "    # Get categorical feature indices for TabNet\n",
    "    cat_idxs = [i for i, col in enumerate(X_train.columns) if col in cat_features]\n",
    "    cat_dims = [int(X_train[col].nunique()) for col in X_train.columns if col in cat_features]\n",
    "    \n",
    "    # Create a simple MSE metric class\n",
    "    class MSE(Metric):\n",
    "        def __init__(self):\n",
    "            self._name = \"mse\"\n",
    "            self._maximize = False\n",
    "\n",
    "        def __call__(self, y_true, y_pred):\n",
    "            y_pred = y_pred.squeeze()\n",
    "            y_true = y_true.squeeze()\n",
    "            return mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    # Initialize simplified TabNet model\n",
    "    model = TabNetRegressor(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        cat_emb_dim=1,\n",
    "        # Simplified architecture\n",
    "        n_d=32,  # Reduced from 64\n",
    "        n_a=32,  # Reduced from 64\n",
    "        n_steps=3,  # Reduced from 5\n",
    "        gamma=1.3,  # Reduced from 1.5\n",
    "        n_independent=1,  # Reduced from 2\n",
    "        n_shared=1,  # Reduced from 2\n",
    "        # Stronger regularization\n",
    "        lambda_sparse=5e-3,  # Increased from 1e-4\n",
    "        # Learning rate and optimizer\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=1e-2),  # Reduced from 2e-2\n",
    "        scheduler_params={\"step_size\":25, \"gamma\":0.75},\n",
    "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "        mask_type='sparsemax',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train_array = X_train.values\n",
    "    X_val_array = X_val.values\n",
    "    \n",
    "    # Train with larger batches for speed\n",
    "    model.fit(\n",
    "        X_train=X_train_array, \n",
    "        y_train=y_train_log,\n",
    "        eval_set=[(X_val_array, y_val_log)],\n",
    "        eval_name=['valid'],\n",
    "        eval_metric=[MSE],\n",
    "        max_epochs=100,\n",
    "        patience=10,  # Reduced patience for faster stopping\n",
    "        batch_size=2048,  # Increased from 1024\n",
    "        virtual_batch_size=256,  # Increased from 128\n",
    "    )\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = test_df.drop(target_col, axis=1) if target_col in test_df.columns else test_df\n",
    "    \n",
    "    # Make predictions\n",
    "    y_test_pred_log = model.predict(X_test.values)\n",
    "    y_test_pred = np.expm1(y_test_pred_log.squeeze())\n",
    "    \n",
    "    # Calculate RMSLE on validation set\n",
    "    if target_col in test_df.columns:\n",
    "        y_test = test_df[target_col]\n",
    "        y_val_pred_log = model.predict(X_val_array)\n",
    "        y_val_pred = np.expm1(y_val_pred_log.squeeze())\n",
    "        val_rmsle = np.sqrt(mean_squared_error(np.log1p(y_val), np.log1p(y_val_pred)))\n",
    "        print(f\"Final Validation RMSLE: {val_rmsle:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feat_importances = pd.DataFrame(\n",
    "        {'feature': X_train.columns, 'importance': model.feature_importances_}\n",
    "    ).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 important features:\")\n",
    "    print(feat_importances.head(10))\n",
    "    \n",
    "    return model, y_test_pred, feat_importances, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f2f4c-265b-4ba6-8f07-5d4975336b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_model, tabnet_predictions, tabnet_feature_importances, label_encoders = train_simplified_tabnet(\n",
    "    nn_train_df=nn_train_df, \n",
    "    nn_test_df=nn_test_df,\n",
    "    target_col='Calories',\n",
    "    cat_features=['Sex']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f2cbdd-c664-42e9-8886-71a39112f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e8cea-2778-4f06-b78e-d2cf30af025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating predictions for test set using TabNet...\")\n",
    "\n",
    "# Apply the same normalization to test_df as we did for training\n",
    "# Assuming you used the same log_cols and excluded_cols as before\n",
    "test_df_normalized = nn_test_df\n",
    "\n",
    "# Manually recreate the label encoders\n",
    "label_encoders = {}\n",
    "\n",
    "# For 'Sex' feature - ensuring the exact same order as training\n",
    "sex_encoder = LabelEncoder()\n",
    "# The ordering shows 'female' first, then 'male'\n",
    "sex_encoder.fit(['female', 'male'])  # This ordering is critical - it must match what was used in training\n",
    "label_encoders['Sex'] = sex_encoder\n",
    "\n",
    "#for cat_col, encoder in label_encoders.items():\n",
    " #   if cat_col in test_df_normalized.columns:\n",
    "  #      test_df_normalized[cat_col] = encoder.transform(test_df_normalized[cat_col].astype(str))\n",
    "\n",
    "# Use the trained TabNet model to make predictions\n",
    "# The model already handles categorical encoding internally via the label_encoders\n",
    "test_predictions_log = tabnet_model.predict(test_df_normalized.values)\n",
    "\n",
    "# Transform back from log space to original scale\n",
    "test_predictions = np.expm1(test_predictions_log.squeeze())  # squeeze() removes extra dimension\n",
    "\n",
    "# Ensure predictions are non-negative\n",
    "test_predictions = np.maximum(0, test_predictions)\n",
    "\n",
    "# Create the submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'id': orig_test_df['id'],\n",
    "    'Calories': test_predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('tabnet_submission.csv', index=False)\n",
    "print(f\"TabNet submission file created: tabnet_submission.csv with {submission.shape[0]} rows\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst few rows of the TabNet submission file:\")\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26677b87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Training XGBoost with robust RMSLE optimization...\")\n",
    "X = train_df.drop('Calories', axis=1)\n",
    "y = train_df['Calories']\n",
    "\n",
    "# Create train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify categorical features\n",
    "cat_features = ['Sex']\n",
    "\n",
    "# Apply one-hot encoding for categorical features (XGBoost doesn't handle categorical features directly like CatBoost)\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=cat_features, drop_first=True)\n",
    "X_val_encoded = pd.get_dummies(X_val, columns=cat_features, drop_first=True)\n",
    "\n",
    "# Log-transform targets\n",
    "y_train_log = np.log1p(np.maximum(0, y_train))\n",
    "y_val_log = np.log1p(np.maximum(0, y_val))\n",
    "\n",
    "# XGBoost configuration - parameters chosen to be similar to the CatBoost setup\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.01,        # Reduced learning rate for stability\n",
    "    max_depth=8,               # Similar depth as CatBoost\n",
    "    objective='reg:squarederror',  # MSE objective for log-transformed data\n",
    "    eval_metric='rmsle',        # Standard RMSE evaluation\n",
    "    random_state=42,\n",
    "    verbosity=1,\n",
    "    reg_lambda=5,              # L2 regularization similar to l2_leaf_reg\n",
    "    min_child_weight=10,       # Similar to min_data_in_leaf\n",
    "    subsample=0.8,             # Add some subsampling for robustness\n",
    "    colsample_bytree=0.8       # Feature subsampling\n",
    ")\n",
    "\n",
    "# Train the model on log-transformed targets with early stopping\n",
    "# Note: early_stopping_rounds should be provided as a parameter to fit_params, not directly to fit()\n",
    "eval_set = [(X_val_encoded, y_val_log)]\n",
    "xgb_model.fit(\n",
    "    X_train_encoded, \n",
    "    y_train_log,\n",
    "    eval_set=eval_set,\n",
    ")\n",
    "\n",
    "# Make predictions (on log scale) and transform back\n",
    "val_predictions_log = xgb_model.predict(X_val_encoded)\n",
    "val_predictions = np.expm1(val_predictions_log)  # expm1 is inverse of log1p\n",
    "\n",
    "# Ensure predictions are non-negative (should already be due to exp transform)\n",
    "val_predictions = np.maximum(0, val_predictions)\n",
    "\n",
    "# Calculate RMSLE directly\n",
    "def rmsle(y_true, y_pred):\n",
    "    # Ensure inputs are positive\n",
    "    y_true = np.maximum(0, y_true)\n",
    "    y_pred = np.maximum(0, y_pred)\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "# Evaluate the model\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "val_rmse = np.sqrt(val_mse)\n",
    "val_rmsle = rmsle(y_val, val_predictions)\n",
    "val_r2 = r2_score(y_val, val_predictions)\n",
    "\n",
    "print(f\"Validation MSE: {val_mse:.2f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.2f}\")\n",
    "print(f\"Validation RMSLE: {val_rmsle:.4f}\")  # This is your target metric\n",
    "print(f\"Validation R²: {val_r2:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "importance = xgb_model.feature_importances_\n",
    "feature_names = X_train_encoded.columns\n",
    "importance_df = sorted(zip(feature_names, importance), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "for name, importance in importance_df:\n",
    "    print(f\"{name}: {importance}\")\n",
    "\n",
    "# If you need to predict on test data later\n",
    "# test_encoded = pd.get_dummies(test_df, columns=cat_features, drop_first=True)\n",
    "# test_predictions_log = xgb_model.predict(test_encoded)\n",
    "# test_predictions = np.expm1(test_predictions_log)\n",
    "# test_predictions = np.maximum(0, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40445001-5b27-4500-aa3e-9c43c3fa1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating predictions for test set...\")\n",
    "X_test = test_df\n",
    "\n",
    "# We need to apply the same one-hot encoding to the test data\n",
    "# First, identify categorical features\n",
    "cat_features = ['Sex']\n",
    "\n",
    "# Apply one-hot encoding for categorical features\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=cat_features, drop_first=True)\n",
    "\n",
    "# Ensure the columns match exactly with training data\n",
    "# Get the columns from the trained model (might be accessible via feature_names_in_)\n",
    "train_columns = X_train_encoded.columns\n",
    "\n",
    "# Check if any columns are missing in the test data\n",
    "missing_cols = set(train_columns) - set(X_test_encoded.columns)\n",
    "# Add missing columns with default value of 0\n",
    "for col in missing_cols:\n",
    "    X_test_encoded[col] = 0\n",
    "    \n",
    "# Ensure columns are in the same order as training data\n",
    "X_test_encoded = X_test_encoded[train_columns]\n",
    "\n",
    "# Make predictions (these are still in log space)\n",
    "test_predictions_log = xgb_model.predict(X_test_encoded)\n",
    "\n",
    "# Transform back from log space to original scale\n",
    "test_predictions = np.expm1(test_predictions_log)  # This is the inverse of log1p\n",
    "\n",
    "# Ensure predictions are non-negative (although expm1 should always give positive values)\n",
    "test_predictions = np.maximum(0, test_predictions)\n",
    "\n",
    "# Create the submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'id': orig_test_df['id'],\n",
    "    'Calories': test_predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Submission file created: submission.csv with {submission.shape[0]} rows\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst few rows of the submission file:\")\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b774c-8a0f-45be-b086-b7a90ce7af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Kaggle or locally\n",
    "IN_KAGGLE = os.path.exists('/kaggle/working')\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    # If running on Kaggle, use the built-in submission mechanism\n",
    "    print(\"Running on Kaggle - please use the 'Submit' button in the UI to submit your results\")\n",
    "else:\n",
    "    # If running locally, use the Kaggle API to submit\n",
    "    print(\"Submitting via Kaggle API...\")\n",
    "    \n",
    "    # Ensure Kaggle API is installed\n",
    "    try:\n",
    "        import kaggle\n",
    "    except ImportError:\n",
    "        print(\"Kaggle API not found. Installing...\")\n",
    "        !pip install kaggle\n",
    "        import kaggle\n",
    "    \n",
    "    # Submit the file\n",
    "    # Note: Make sure you have Kaggle API credentials set up (~/.kaggle/kaggle.json)\n",
    "    competition_name = \"playground-series-s5e5\"\n",
    "    submission_message = \"xgboost with feature engineering\"\n",
    "    \n",
    "    # Command to submit \n",
    "    submission_command = f\"kaggle competitions submit -c {competition_name} -f submission.csv -m \\\"{submission_message}\\\"\"\n",
    "    \n",
    "    print(f\"Running command: {submission_command}\")\n",
    "    !{submission_command}\n",
    "    \n",
    "    # Check your submissions (optional)\n",
    "    print(\"\\nYour recent submissions:\")\n",
    "    !kaggle competitions submissions -c {competition_name}\n",
    "    \n",
    "print(\"\\nDone! 🎉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b108a0-e67a-4eea-9505-7d5df78683fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit tabnet\n",
    "\n",
    "# Check if running on Kaggle or locally\n",
    "IN_KAGGLE = os.path.exists('/kaggle/working')\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    # If running on Kaggle, use the built-in submission mechanism\n",
    "    print(\"Running on Kaggle - please use the 'Submit' button in the UI to submit your results\")\n",
    "else:\n",
    "    # If running locally, use the Kaggle API to submit\n",
    "    print(\"Submitting via Kaggle API...\")\n",
    "    \n",
    "    # Ensure Kaggle API is installed\n",
    "    try:\n",
    "        import kaggle\n",
    "    except ImportError:\n",
    "        print(\"Kaggle API not found. Installing...\")\n",
    "        !pip install kaggle\n",
    "        import kaggle\n",
    "    \n",
    "    # Submit the file\n",
    "    # Note: Make sure you have Kaggle API credentials set up (~/.kaggle/kaggle.json)\n",
    "    competition_name = \"playground-series-s5e5\"\n",
    "    submission_message = \"pytorch tabnet\"\n",
    "    \n",
    "    # Command to submit \n",
    "    submission_command = f\"kaggle competitions submit -c {competition_name} -f tabnet_submission.csv -m \\\"{submission_message}\\\"\"\n",
    "    \n",
    "    print(f\"Running command: {submission_command}\")\n",
    "    !{submission_command}\n",
    "    \n",
    "    # Check your submissions (optional)\n",
    "    print(\"\\nYour recent submissions:\")\n",
    "    !kaggle competitions submissions -c {competition_name}\n",
    "    \n",
    "print(\"\\nDone! 🎉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3261f4-4402-4a66-8b82-9eee65330870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastAI (GPU)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
